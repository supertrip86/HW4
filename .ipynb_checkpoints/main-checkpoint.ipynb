{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eugenio Baldo, Giovanni Giunta and  Leonardo Masci - Group 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this first exercise was to gain an understanding of hashing and the HyperLogLog algorithm, the state of the art when it comes to estimating the number of unique users.\n",
    "In order to accomplish this task we have consulted a number of resources, including the following paper ( http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf ) from which we got a lot of the formulas we then implemented in our HLL function.\n",
    "\n",
    "Our work for this exercise is articulated in three steps:\n",
    "1. Transform the given dataset into a set of binaries\n",
    "2. Implement our HLL function\n",
    "3. Estimate the number of unique elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the binary file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we import everything we need and visualise the given initial txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import MODULES4 as mod\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD as SVD\n",
    "from wordcloud import WordCloud as WC\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans\n",
    "import itertools\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>844082e02a27ddee8d99ea1af94a2969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ff96d6665b5c59d3a70bb8f2ba4f10be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b64a85884e2b159829331c19e05dbac9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1c8836719e84867c26ba2cfeb372c53d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b66f73ffd9008d9c99159e164261df51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  0\n",
       "0  844082e02a27ddee8d99ea1af94a2969\n",
       "1  ff96d6665b5c59d3a70bb8f2ba4f10be\n",
       "2  b64a85884e2b159829331c19e05dbac9\n",
       "3  1c8836719e84867c26ba2cfeb372c53d\n",
       "4  b66f73ffd9008d9c99159e164261df51"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('hash.txt', header=None)\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is comprised of hexadecimal numbers. Therefore, in order to create our binary file we simply had to convert those numbers into binaries. A problem we encountered with this step was that not all the output values were of the same length, which created problems in the following steps. For this reason, we decided to add 0s as needed to reach the desired length for each binary value. We included the results in a txt file to avoid having to recreate the dataset every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"binaries.txt\", \"w\")\n",
    "\n",
    "for i, row in dataset.iterrows():            \n",
    "    bin_value = bin(int(row[0], 16))[2:].zfill(len(row[0]) * 4)\n",
    "    f.write(bin_value + '\\n')\n",
    "\n",
    "    if (i % 10000000) == 0:\n",
    "        print(i)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000010001000000100000101110000000101010001001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1111111110010110110101100110011001011011010111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1011011001001010100001011000100001001110001010...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001110010001000001101100111000110011110100001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1011011001101111011100111111111111011001000000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  1000010001000000100000101110000000101010001001...\n",
       "1  1111111110010110110101100110011001011011010111...\n",
       "2  1011011001001010100001011000100001001110001010...\n",
       "3  0001110010001000001101100111000110011110100001...\n",
       "4  1011011001101111011100111111111111011001000000..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binaries = pd.read_csv('binaries.txt', header=None)\n",
    "\n",
    "binaries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing our HyperLogLog function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the size of the binaries and the amount of values we were working with, we decided that 16 would be a good number of bytes to determine the destination bucket of each value, as we would end up with ${2^{16}}$ buckets. We also initialised a dictionary that had 0 as the value for every single bucket.\n",
    "\n",
    "We then counted the number of 0s found in each string binary value until the first 1 was encountered. Lastly, we confronted this number with the number already present for the corresponding bucket: if the new value was higher we substituted it. This way we ended up with a dictionary containing the highest amount of consecutive 0s for each bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 16\n",
    "values = [int(\"\".join(map(str, list(i))), 2) for i in itertools.product([0, 1], repeat=b)]\n",
    "db = {x: 0 for x in values}\n",
    "\n",
    "def hyperLogLog(binary, b):\n",
    "    bucket = int(binary[:b], 2)\n",
    "    rest = binary[b:]\n",
    "    count = 0\n",
    "    for char in rest:\n",
    "        if char == '0':\n",
    "            count += 1\n",
    "        else:\n",
    "            count += 1\n",
    "            break\n",
    "\n",
    "    if db[bucket] < count:\n",
    "        db[bucket] = count\n",
    "\n",
    "    if (i % 10000000) == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the function was ready, we applied it to our binary dataset and saved the resulting dictionary as a json file, again for easy future access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in binaries.iterrows():\n",
    "    binary = row[0]\n",
    "    hyperLogLog(binary, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"db.json\", \"w\") as outfile: \n",
    "    json.dump(db, outfile, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('db.json') as f:\n",
    "    dt = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating amount of unique elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the formulas we found in the above mentioned paper, we calculated the following values based on our final dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_z(values):\n",
    "    z = 0\n",
    "    for x in values:\n",
    "        z += 2 ** (-x)\n",
    "    return z\n",
    "\n",
    "m = len(dt)\n",
    "values = list(dt.values())\n",
    "z = calc_z(values)\n",
    "alpha = 0.7213/(1 + 1.079 / m )\n",
    "E = round((alpha * float(m**2)) / z)\n",
    "error = 1.04 / (math.sqrt(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125674524"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0040625"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we estimated that the cardinality of our initial dataset was of about 125 million, with an approximated error of 0.004."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "### Clustering of reviews of fine foods bought on amazon #\n",
    "\n",
    "First, we import and clean the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                         0\n",
       "ProductId                  0\n",
       "UserId                     0\n",
       "ProfileName               16\n",
       "HelpfulnessNumerator       0\n",
       "HelpfulnessDenominator     0\n",
       "Score                      0\n",
       "Time                       0\n",
       "Summary                   27\n",
       "Text                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"Reviews.csv\", header=\"infer\")\n",
    "\n",
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"ProfileName\"].fillna(\"\",inplace=True)\n",
    "df1[\"Summary\"].fillna('',inplace=True)\n",
    "df1.set_index(\"Id\", inplace=True)\n",
    "df1['Datetime'] = df1.Time.apply(lambda value: pd.to_datetime(value, unit='s'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use the NLTK module to process the review plain text, then we create column with all the tokens and we add it to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\giogi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\giogi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\giogi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>taken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>[bought, several, vitality, canned, dog, food,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>[product, arrived, labeled, jumbo, salted, pea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>[confection, around, century, light, pillowy, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>[looking, secret, ingredient, robitussin, beli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>[great, taffy, great, price, wide, assortment,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \\\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...   \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...   \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...   \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...   \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...   \n",
       "\n",
       "                                         taken_words  \n",
       "0  [bought, several, vitality, canned, dog, food,...  \n",
       "1  [product, arrived, labeled, jumbo, salted, pea...  \n",
       "2  [confection, around, century, light, pillowy, ...  \n",
       "3  [looking, secret, ingredient, robitussin, beli...  \n",
       "4  [great, taffy, great, price, wide, assortment,...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "wordlist_1 = df1[\"Text\"].apply(lambda text: mod.new_text(text,stop_words)).to_list()\n",
    "df1[\"taken_words\"] = wordlist_1\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to cluster the reviews text by taking into account only the adjectives. This choice is related to the role played by adjectives in each sentence. The use of a particular mix of adjectives gives a specific connotation to the text. It has been decided to use as columns only the first 1000 most popular adjectives in the reviews. This allows to consider general adjectives and not adjectives that are too specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_list = df1[\"taken_words\"].apply(lambda wordlist_1: mod.take_adj(wordlist_1))\n",
    "df1[\"adjectives\"] = adj_list\n",
    "list_agg = adj_list.to_list()\n",
    "flat_list_adj = [item for sublist in adj_list for item in sublist]\n",
    "adjectives_list = pd.Series(flat_list_adj).value_counts()\n",
    "all_adjectives = list(adjectives_list[0:1000].index)\n",
    "\n",
    "len(all_adjectives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a dictionary where reviews are the keys and vectors of size 1000 are the values. In order to represent a particular review text has been decided to use a binary representation for each adjective considered. If the adjective is in the review text, it will be assigned 1, otherwise it will be assigned 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_adj = mod.df_words_dict(list_agg,all_adjectives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**here we create a dataset where each row is a review as in the starting one, whereas the columns are the adjectives.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['good', 'great', 'little', 'much', 'many', 'delicious', 'sweet', 'free',\n",
       "       'nice', 'hot',\n",
       "       ...\n",
       "       'seasonal', 'routine', 'toxic', 'boring', 'wait', 'go', 'kona',\n",
       "       'definite', 'ground', 'pamela'],\n",
       "      dtype='object', length=1000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_adjectives = pd.DataFrame.from_dict(data=dict_adj, orient='index',columns=all_adjectives)\n",
    "\n",
    "df_adjectives.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVD method to reduce the dimensionality** \n",
    "\n",
    "The SVD method is used to reduce the number of features which will be used for clustering and in general for machine learning models. This method allows to save computation time but at the same time to save a good amount of information provided by the features. It has been set a target value for the variance retained of 60%. So it will be choosen a number of components which is capable of retaining 60% of all the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.615875475258968\n"
     ]
    }
   ],
   "source": [
    "svd_agg = SVD(n_components=130, n_iter=7, random_state=42)\n",
    "df_adj = svd_agg.fit_transform(df_adjectives, y=None)\n",
    "\n",
    "print(svd_agg.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "having the number of components to be used, they will be taken the top features ordered in descending order by variance. These features will approximately correspond to the most popular adjectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>good</th>\n",
       "      <th>great</th>\n",
       "      <th>little</th>\n",
       "      <th>much</th>\n",
       "      <th>delicious</th>\n",
       "      <th>many</th>\n",
       "      <th>nice</th>\n",
       "      <th>favorite</th>\n",
       "      <th>sweet</th>\n",
       "      <th>different</th>\n",
       "      <th>...</th>\n",
       "      <th>difficult</th>\n",
       "      <th>want</th>\n",
       "      <th>particular</th>\n",
       "      <th>vegetable</th>\n",
       "      <th>live</th>\n",
       "      <th>flavor</th>\n",
       "      <th>gluten</th>\n",
       "      <th>pleased</th>\n",
       "      <th>tiny</th>\n",
       "      <th>super</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568449</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568450</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568451</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568452</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568453</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>568454 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        good  great  little  much  delicious  many  nice  favorite  sweet  \\\n",
       "0          1      0       0     0          0     0     0         0      0   \n",
       "1          0      0       0     0          0     0     0         0      0   \n",
       "2          0      0       0     0          0     0     0         0      0   \n",
       "3          1      0       0     0          0     0     0         0      0   \n",
       "4          0      1       0     0          0     0     0         0      0   \n",
       "...      ...    ...     ...   ...        ...   ...   ...       ...    ...   \n",
       "568449     1      1       0     0          0     0     0         0      0   \n",
       "568450     0      0       0     0          0     0     0         0      0   \n",
       "568451     0      0       1     0          0     0     0         0      1   \n",
       "568452     1      0       0     0          0     0     0         1      1   \n",
       "568453     0      0       0     0          0     0     0         0      0   \n",
       "\n",
       "        different  ...  difficult  want  particular  vegetable  live  flavor  \\\n",
       "0               0  ...          0     0           0          0     0       0   \n",
       "1               0  ...          0     0           0          0     0       0   \n",
       "2               0  ...          0     0           0          0     0       0   \n",
       "3               0  ...          0     0           0          0     0       0   \n",
       "4               0  ...          0     0           0          0     0       0   \n",
       "...           ...  ...        ...   ...         ...        ...   ...     ...   \n",
       "568449          0  ...          0     0           0          0     0       0   \n",
       "568450          0  ...          0     0           0          0     0       0   \n",
       "568451          0  ...          0     0           0          0     0       0   \n",
       "568452          0  ...          0     0           0          0     0       0   \n",
       "568453          0  ...          0     0           0          0     0       0   \n",
       "\n",
       "        gluten  pleased  tiny  super  \n",
       "0            0        0     0      0  \n",
       "1            0        0     0      0  \n",
       "2            0        0     1      0  \n",
       "3            0        0     0      0  \n",
       "4            0        0     0      0  \n",
       "...        ...      ...   ...    ...  \n",
       "568449       0        0     0      0  \n",
       "568450       0        0     0      0  \n",
       "568451       0        0     0      0  \n",
       "568452       0        0     0      0  \n",
       "568453       0        0     0      0  \n",
       "\n",
       "[568454 rows x 120 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_variance = mod.variance_columns(df_adjectives,130)\n",
    "df_adjectives = df_adjectives[columns_variance]\n",
    "\n",
    "df_adjectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elbow method for number of cluster selection**\n",
    "\n",
    "The number of clusters can be utmost the total number of rows. Given certain  boundaries, the more are the cluster the more specific will be the clustering. However an high number of clusters can raise computational problems. To choose an acceptable number of clusters, it will be used the elbow method which computes the inertia for a specific cluster number. From the following plot, it is possible to observe the inertia curve for clusters numbers in range(2,30). the first derivative of this curve appears less negative after a cluster number of 14. So, it will be used 14 as cluster number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 28/28 [20:52<00:00, 44.73s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "elbow = {}\n",
    "for k in tqdm(range(2, 30)):\n",
    "    model = KMeans(k)\n",
    "    model.fit(df_adjectives)\n",
    "    model.predict(df_adjectives)    \n",
    "    elbow[k] = model.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.xlabel('cluster number')\n",
    "plt.ylabel('inertia')\n",
    "plt.plot(list(elbow.keys()), list(elbow.values()), color='red', linestyle='dotted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nested list to be used for our own clustering implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector1 = []\n",
    "\n",
    "for idx in range(len(df_adjectives)):\n",
    "    list_w1 = list(df_adjectives.iloc[idx])\n",
    "    word_vector1.append(list_w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section two k-means algorithm will be executed. The first is our own algorithm with random initialization, the second is the one provided from sk-learn with k++ initialization.\n",
    "\n",
    "**Our k-means implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster labels for iteration 0 for the first 100 items are [4, 12, 12, 4, 1, 12, 1, 4, 12, 4, 1, 12, 12, 4, 12, 12, 4, 12, 8, 12, 12, 12, 12, 12, 12, 12, 12, 1, 1, 1, 4, 1, 4, 4, 4, 0, 12, 12, 0, 5, 4, 4, 1, 4, 4, 12, 4, 4, 5, 12, 4, 1, 1, 12, 2, 12, 12, 1, 1, 12, 4, 4, 12, 1, 12, 12, 12, 12, 4, 12, 4, 4, 4, 4, 12, 12, 4, 4, 1, 4, 12, 1, 1, 8, 12, 1, 12, 12, 8, 4, 12, 1, 4, 8, 12, 12, 12, 12, 12, 12]\n",
      "cluster labels for iteration 1 for the first 100 items are [0, 12, 12, 4, 1, 12, 1, 4, 12, 4, 12, 12, 12, 4, 12, 12, 4, 12, 8, 12, 12, 12, 12, 12, 12, 12, 12, 1, 1, 12, 4, 1, 4, 4, 4, 5, 12, 12, 0, 5, 4, 4, 1, 4, 4, 12, 4, 4, 5, 12, 4, 12, 12, 12, 2, 10, 12, 1, 1, 12, 4, 4, 12, 1, 12, 12, 12, 12, 4, 12, 4, 4, 4, 4, 12, 12, 4, 4, 1, 4, 12, 1, 3, 8, 12, 1, 12, 12, 8, 4, 12, 1, 4, 8, 12, 12, 12, 12, 12, 12]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-d142fba03b7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmy_clusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_vector1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Data Science\\Algorithmic Methods of Data Mining\\HW4\\MODULES4.py\u001b[0m in \u001b[0;36mcluster_labels\u001b[1;34m(word_vector, iterations, cluster_number)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                 \u001b[0mdist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meuclidean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcenter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py\u001b[0m in \u001b[0;36meuclidean\u001b[1;34m(u, v, w)\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m     \"\"\"\n\u001b[1;32m--> 614\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mminkowski\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py\u001b[0m in \u001b[0;36mminkowski\u001b[1;34m(u, v, p, w)\u001b[0m\n\u001b[0;32m    504\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"p must be at least 1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m     \u001b[0mu_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mu\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "my_clusters = mod.cluster_labels(word_vector1,30,14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sklearn with k++ initialization results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=14,init='k-means++', max_iter=30, verbose=False).fit(df_adjectives)\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "print(clusters[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster values for both algorithms are added to the starting dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"own_cluster\"] = my_clusters\n",
    "df1[\"sklearn_cluster_k++\"] = clusters\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = pd.Series(clusters).value_counts().sort_values().to_dict()\n",
    "my_clusters = pd.Series(my_clusters).value_counts().sort_values().to_dict()\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.title(\"clusters frequencies using our implementation\".upper())\n",
    "plt.bar(my_clusters.keys(),my_clusters.values(),color='yellow')\n",
    "plt.xticks(np.arange(0,14,1))\n",
    "plt.yticks(np.arange(0,40000,1000))\n",
    "plt.xlabel(\"cluster\")\n",
    "plt.ylabel('items in cluster')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.title('clusters frequencies using sk-learn'.upper())\n",
    "plt.bar(clusters.keys(),clusters.values(), color='blue')\n",
    "plt.xticks(np.arange(0,14,1))\n",
    "plt.yticks(np.arange(0,40000,1000))\n",
    "plt.xlabel(\"cluster\")\n",
    "plt.ylabel('items in cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous histograms it is possible to observe the number of reviews for each cluster. The cluster labels of our implementation and sk-learn will not correspond each other, given that the initialization of centers changes each time that the script is executed. It can be observed that for both implementations there exists a major cluster that has the higher number of reviews. Then there are two middle clusters which have approximately the same number in our implementation and slightly difference in sk-learn and finally there are minor clusters. The number of items for the two implementations for each cluster results not equal but looks similar. In addition to graphs, it is possible to see, from the sequences of labels patterns, many common sequences between the two results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster analysis\n",
    "\n",
    "Provide the number of products for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count = df1.groupby([\"own_cluster\"]).count()\n",
    "df_count = df_count.ProductId\n",
    "\n",
    "plt.pie(df_count,labels=df_count.index,radius=3,autopct='%1.1f%%',title=\"Percentages of products per cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = mod.score_distribution(df1,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = list(score[0].values())\n",
    "stds = list(score[1].values())\n",
    "print(means)\n",
    "print(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind_from_stats as ttest\n",
    "\n",
    "mean_dataset = np.mean(df1.Score)\n",
    "std_dataset = np.std(df1.Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEAN DIFFERENCE SIGNIFICANCY AMONG THE SCORES OF ALL DATASET AND THE SCORES OF EACH CLUSTER\n",
    "\n",
    "mod.mean_difference_main(0.05,means,stds,df1,df_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEAN DIFFERENCE AMONG ALL THE COMBINATIONS(n=14,k=2) OF CLUSTERS\n",
    "\n",
    "mod.mean_difference(0.05,means,stds,df_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the mean values for each cluster and the standard deviation of the means, it is reasonable to claim that the mean difference among clusters is not statistically significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "plt.bar(list(score.keys()),means,color='salmon')\n",
    "plt.xlabel(\"cluster number\")\n",
    "plt.xticks(np.arange(0,14,1))\n",
    "plt.yticks(np.arange(0,5,0.2))\n",
    "plt.ylabel('score mean')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of unique users writing reviews in each cluster\n",
    "\n",
    "uniques = mod.unique_users(df1, 14)\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.title('unique users for each cluster writing reviews')\n",
    "plt.barh(uniques.index, uniques, color='orange')\n",
    "plt.yticks(np.arange(0,14,1))\n",
    "plt.xticks(np.arange(0,20000,1000))\n",
    "plt.xlabel('unique users')\n",
    "plt.ylabel('cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud\n",
    "\n",
    "In this section will be displayed 3 different wordclouds. Our group has not suceeded to find a particular library which provide food items. So, with the first 2 wordclouds we have tried 2 different approaches: the first was to take all the nouns into account with nltk tagging, as done before with the adjectives. However with this approach are considered nouns that are not food at all, in addition to wrong nltk tags. With the second word cloud we have created manually a list with a subset of foods, which, looking empirically to the reviews, should be among the most bought. This approach looses a lot of information, but gives categories which belong to food and beverages.\n",
    "\n",
    "in the third word cloud, our group has visualized the most popular adjectives in each cluster. when this wordcloud is compared with the previous cloud, it is possible to see a few associations among the most popular products in each cluster and the most popular adjectives in the same cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud as WC\n",
    "\n",
    "coffee_image = np.array(Image.open('moka.jpg'))\n",
    "df1.Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_list = ['tea','coffee','chocolate','water','cocoa','sauce','juice','ice cream','cake','butter','bread','milk','pizza','pasta','eggs','cookies','meat','soup','ramen','noodles','beans','popcorn','corn','broccoli','tuna','salmon','bacon','coke','onion','nuggets','potatoes','salt','pepper','sugar','almonds','nuts','peanuts','flour','rice','wine','beer','chicken','pork']\n",
    "names_list = df1[\"taken_words\"].apply(lambda wordlist_1: mod.take_names(wordlist_1))\n",
    "flat_list_nn = [item for sublist in names_list for item in sublist]\n",
    "nouns_list = pd.Series(flat_list_nn).value_counts().index\n",
    "\n",
    "nouns_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First word cloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.word_cloud(coffee_image,df1,14,nouns_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second word cloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.word_cloud(coffee_image,df1,14,food_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Third word cloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.word_cloud_1(coffee_image,df1,14,all_adjectives)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
